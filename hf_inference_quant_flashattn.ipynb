{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace LLM inference experiments\n",
    "\n",
    "- Compare inference options for quantisation and flash-atten\n",
    "- As of 01/2024 torch's built in sdpa attention impmenentation has flash-attention without the need for the external lib from Tri Dao (which can be a pain to install). But it's not currently implemented within HF for all models -- particularly there's no support for Mistral\n",
    "- So still need to use flash-atten lib for now to get flash-attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First just load Mistral model with no additional options -- would be just as easy to use their reference implementation but HF gives all the other options for quant etc so go with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793, 28705]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793, 28705,   315,   949, 28742, 28707,\n",
      "           506,   264,  9230,  5369, 28713,   442,   264, 21448,   354,  2076,\n",
      "          8447, 28723,  2993, 28725,   315,   541,  1912,   368,   369,   741,\n",
      "          3298, 10392,  3387,  3024,   446,  4455,   715, 28725,  1580,   488,\n",
      "         28725,   993,  7136,   864, 28725,  3296, 16042, 28725,   579, 28724,\n",
      "         16042, 28725,   304, 26678, 21993, 28723,   661, 12282,  9618,   356,\n",
      "          3327,  9230, 22731,   304,   272,  1212,   302,  2887,  1250, 23440,\n",
      "         28723,     2]], device='cuda:0')\n",
      "[\"[INST] What is your favourite condiment? [/INST]  I don't have a taste buds or a preference for condiments. However, I can tell you that some common favourites include ketchup, mustard, mayonnaise, hot sauce, soy sauce, and ranch dressing. It largely depends on personal taste preferences and the type of food being consumed.\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What is your favourite condiment? [/INST] \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "print(generated_ids)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How were the samples in the sp1200 encoded?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The samples were encoded in 12-bit PCM.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What could be done to improve the drum sounds?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,\n",
       "           272,   668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748,\n",
       "         16289, 28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750,\n",
       "         28733,  2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,\n",
       "           829,   347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733,\n",
       "         28748, 16289, 28793]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,   272,\n",
      "           668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748, 16289,\n",
      "         28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750, 28733,\n",
      "          2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,   829,\n",
      "           347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733, 28748,\n",
      "         16289, 28793]], device='cuda:0')\n",
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST]There are several ways to improve the drum sounds from an SP-1200 or any other sampler using similar technology:\n",
      "\n",
      "1. Updating or replacing the sampler's internal samples: The drum sounds that came with the SP-1200 were recorded in the late 1980s and early 1990s, so updating or replacing them with modern sample libraries or recordings of higher quality could significantly improve the overall sound.\n",
      "2. Equalization: Adjusting the equalizer settings can help bring out the desired frequencies and tame problematic ones. For example, boosting the high-frequency range can help make the drums sound more snappy and clear, while cutting the low-end can make the drums sound tighter and more defined.\n",
      "3. Compression: Applying compression can help even out the dynamics of the drums, making them sound more consistent and punchy. This effect is particularly useful for drum sounds that have a lot of variability in volume, such as snares and hi-hats.\n",
      "4. Reverb: Adding a touch of reverb to the drums can help make them sound more natural and spacious. However, too much reverb can make\n"
     ]
    }
   ],
   "source": [
    "# encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "print(encodeds)\n",
    "generated_ids = model.generate(encodeds, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as above but without direct tokenization using the chat template...does make a minor difference to the output but can be more flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,\n",
      "           272,   668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748,\n",
      "         16289, 28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750,\n",
      "         28733,  2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,\n",
      "           829,   347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733,\n",
      "         28748, 16289, 28793]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST]There are several ways to improve drum sounds in a digital audio workstation (DAW) or a drum machine like the SP-1200:\n",
      "\n",
      "1. Sampling higher-quality sources: Using higher-quality samples to begin with can greatly improve the sound of your drums. You can record your own drum sounds using high-end microphones and pre-amps, or purchase professionally recorded samples from sample libraries.\n",
      "2. Processing and editing: Applying various processing effects, such as compression, equalization, and reverb, can help enhance the tone and character of your drums. You can also edit the samples to remove unwanted noise, correct inconsistencies in timing, and adjust the volume envelopes to give the drums a more natural attack and decay.\n",
      "3. Layering and blending: Layering and blending multiple samples of the same drum sound can help add depth and complexity to the drum part. For example, you can use a close microphone recording for the transient attack and a room microphone recording for the ambience.\n",
      "4. Tuning and quantizing: Tuning the pitch of drum samples can help them blend better with other sounds in your arrangement. Quantizing the rhythm of the drum samples can\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.2 s ± 304 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 2 model.generate(**inputs, max_new_tokens=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 16.44 t/s: 18.80\n",
      "duration: 16.49 t/s: 18.74\n",
      "Averages for prompt and generated | duration: 16.47 t/s: 18.77\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_repeats = 2\n",
    "total_duration = 0\n",
    "total_tps = 0\n",
    "for _ in range(n_repeats):\n",
    "    start = time.time()\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    total_duration += duration\n",
    "    total_tps += len(generated_ids[0]) / duration\n",
    "    print(f\"duration: {duration:.2f}, t/s: {len(generated_ids[0]) / duration:.2f}\")\n",
    "print(f\"Averages for prompt and generated | duration: {total_duration / n_repeats:.2f}, t/s: {total_tps / n_repeats:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this ignores the difference between prompt and generated tokens, but as long as we're comparing like for like\n",
    "# it's good enough for these purposes. Should find the time for each part separately for a more accurate comparison.\n",
    "def calculate_average_duration(inputs, model, n_repeats=2, max_new_tokens=256):\n",
    "    total_duration = 0\n",
    "    total_tps = 0\n",
    "    for _ in range(n_repeats):\n",
    "        start = time.time()\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        total_duration += duration\n",
    "        total_tps += len(generated_ids[0]) / duration\n",
    "        print(f\"duration: {duration:.2f}, t/s: {len(generated_ids[0]) / duration:.2f}\")\n",
    "    print(f\"Averages for prompt and generated | duration: {total_duration / n_repeats:.2f}, t/s: {total_tps / n_repeats:.2f}\")\n",
    "\n",
    "calculate_average_duration(inputs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test for 2 repeats on the inputs defined above gives:\n",
    "\n",
    "- duration: 16.44 t/s: 18.80\n",
    "- duration: 16.49 t/s: 18.74\n",
    "- Averages for prompt and generated | duration: 16.47 t/s: 18.77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████| 3/3 [00:04<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# check that the variable `model` exists and if it does, delete it\n",
    "if \"model\" in locals():\n",
    "    model = model.to(\"cpu\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST]There are several ways to improve drum sounds from an Akai S1200 or any other sampler, especially when working with older drum sounds:\n",
      "\n",
      "1. Re-sampling: You can re-sample the original drum sounds with higher-quality equipment or software, allowing you to maintain the characteristic sound while improving the overall clarity and resolution.\n",
      "\n",
      "2. Editing: Use an audio editor (such as Adobe Audition, Reaper, or Audacity) to edit the drum sounds. Remove unwanted noise, adjust volume levels, and equalize the EQ to improve the overall tone and balance of the drums.\n",
      "\n",
      "3. Layering: Layer multiple sounds or samples of the same drum sound to create a fuller, more complex drum hit.\n",
      "\n",
      "4. Compression: Use a compressor to even out the dynamic range and add punch to the drums.\n",
      "\n",
      "5. Reverb: Apply a room or ambient reverb effect to the drums to enhance the sense of space and depth.\n",
      "\n",
      "6. Distortion: Adding a controlled amount of distortion can add character and texture to the drums, especially electronic or synthetic drum sounds.\n",
      "\n",
      "7. Adding EQ: Apply EQ to boost\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "calculate_average_duration(inputs, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
