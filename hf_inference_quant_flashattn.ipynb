{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace LLM inference experiments\n",
    "\n",
    "- Compare inference options for quantisation and flash-atten\n",
    "- As of 01/2024 torch's built in sdpa attention impmenentation has flash-attention without the need for the external lib from Tri Dao (which can be a pain to install). But it's not currently implemented within HF for all models -- particularly there's no support for Mistral\n",
    "- So still need to use flash-atten lib for now to get flash-attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First just load Mistral model with no additional options -- would be just as easy to use their reference implementation but HF gives all the other options for quant etc so go with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████| 3/3 [00:06<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793, 28705]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "[\"[INST] What is your favourite condiment? [/INST] I don't have a favourite condiment as I don't consume food or condiments. However, some common favourites among people include ketchup, mayonnaise, hot sauce, soy sauce, and mustard.\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What is your favourite condiment? [/INST] \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793,  6824, 28725,   315, 28742, 28719,\n",
      "          3448, 10473,   298,   264,  1179, 11322, 19961,   302,  6138, 23598,\n",
      "         18342, 28723,   661, 13633,   776,   272,  1103,  3558,   302,   686,\n",
      "         16944, 15637,   423,   298,  5681,   315, 28742, 28719, 13198,   582,\n",
      "           297,   272,  6132, 28808,     2,   733, 16289, 28793,  2378,   368,\n",
      "           506,   993,  7136,   864, 21116, 28804,   733, 28748, 16289, 28793]],\n",
      "       device='cuda:0')\n",
      "[\"[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen! [INST] Do you have mayonnaise recipes? [/INST] Yes, I have several mayonnaise recipes I'd be happy to share with you! Here's a classic one:\\n\\n**Classic Mayonnaise**\\n\\n* 1 egg yolk\\n* 1 tbsp Dijon mustard\\n* 1 cup vegetable oil\\n* 1 tbsp white wine vinegar\\n* 1 tbsp fresh lemon juice\\n* 1 tsp salt\\n\\n1. In a large bowl, whisk together the egg yolk and mustard.\\n2. Starting with a very slow drizzle, add the oil to the egg mixture, whisking constantly to create an emulsion. As the mayonnaise starts to thicken, you can gradually add the oil in a thin stream. If the mayonnaise gets too thick, thin it out with a little water.\\n3. Once all the oil has been incorporated, whisk in the vinegar, lemon juice, and salt until combined.\\n\\nGive it a try and let me know how it turns out! If you're not comfortable using raw eggs, there are also recipes for mayonnaise that use pasteurized eggs or no eggs at all. Just let me know if you'd like to see\"]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "print(encodeds)\n",
    "generated_ids = model.generate(encodeds, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': device(type='cuda')}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
