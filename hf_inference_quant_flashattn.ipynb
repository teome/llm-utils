{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace LLM inference experiments\n",
    "\n",
    "- Compare inference options for quantisation and flash-atten\n",
    "- As of 01/2024 torch's built in sdpa attention impmenentation has flash-attention without the need for the external lib from Tri Dao (which can be a pain to install). But it's not currently implemented within HF for all models -- particularly there's no support for Mistral\n",
    "- So still need to use flash-atten lib for now to get flash-attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/dom/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First just load Mistral model with no additional options -- would be just as easy to use their reference implementation but HF gives all the other options for quant etc so go with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 596/596 [00:00<00:00, 3.11MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 25.1k/25.1k [00:00<00:00, 52.8MB/s]\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.94G/4.94G [00:21<00:00, 233MB/s]\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 5.00G/5.00G [00:22<00:00, 225MB/s]\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 4.54G/4.54G [00:20<00:00, 217MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [01:04<00:00, 21.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.17s/it]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 235kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.54MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 396MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 16.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 313kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793, 28705]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
      "         28804,   733, 28748, 16289, 28793, 28705,   315,   949, 28742, 28707,\n",
      "           506,   264,  5277,  2187,   442,  3327, 22731, 28723,  2993, 28725,\n",
      "           315,   541,  1912,   368,   369,  1287,   905,  1401,   272,  1526,\n",
      "          3555,  4118,  2076,  8447, 10085,   356,   652,  8932,  5414, 28713,\n",
      "           304,  3327, 26918, 28723,  2909,  3298, 10392,  3387,  3024,   446,\n",
      "          4455,   715, 28725,  1580,   488, 28725,   579, 28724, 16042, 28725,\n",
      "          3296, 16042, 28725,   993,  7136,   864, 28725,   304,  1016,   789,\n",
      "         28723, 19576,  9807, 28725,   272,  1489,  2076,  2487,   349,   264,\n",
      "          3209,   302,  3327, 21448, 28723,     2]], device='cuda:0')\n",
      "[\"[INST] What is your favourite condiment? [/INST]  I don't have a physical body or personal preferences. However, I can tell you that many people around the world enjoy various condiments depending on their cultural backgrounds and personal tastes. Some common favourites include ketchup, mustard, soy sauce, hot sauce, mayonnaise, and relish. Ultimately, the best condiment is a matter of personal preference.\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"[INST] What is your favourite condiment? [/INST] \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "print(generated_ids)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How were the samples in the sp1200 encoded?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The samples were encoded in 12-bit PCM.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What could be done to improve the drum sounds?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_chat_template` doesn't return `attention_mask` which isn't really a problem here, but we can either generate it manually or first create a string, then tokenize -- this is advised against, but under the hood is probably what `apply_chat_template` is doing anyway...\n",
    "\n",
    "For llama and mistral, it's easy to use their implementation of sentencepiece tokenizer (see llm-utils for utility funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,\n",
       "           272,   668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748,\n",
       "         16289, 28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750,\n",
       "         28733,  2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,\n",
       "           829,   347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733,\n",
       "         28748, 16289, 28793]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,   272,\n",
      "           668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748, 16289,\n",
      "         28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750, 28733,\n",
      "          2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,   829,\n",
      "           347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733, 28748,\n",
      "         16289, 28793]], device='cuda:0')\n",
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST]There are several ways to improve the drum sounds from an SP-1200:\n",
      "\n",
      "1. Use higher quality samples: Samples with better recordings, more detail and less noise can improve the overall sound quality.\n",
      "2. Editing and processing: The drum sounds can be edited and processed using various techniques such as compressing, equalizing, or adding reverb and delay to enhance their characteristics.\n",
      "3. Velocity layering: Assigning different samples to the pads based on the velocity of the input can add more nuance and expression to the drum sounds.\n",
      "4. Tuning and pitching: Adjusting the pitch or tuning of individual samples can help match them better with the rest of the music and improve overall sound.\n",
      "5. Sequencing and arrangement: The way the drum sounds are sequenced and arranged can have a significant impact on the overall feel and groove of the track. Playing with the timing, accents, and fills can help create more interesting and dynamic drum patterns.\n",
      "6. Using external hardware: Connecting external hardware such as a sampler, synthesizer or drum machine to the SP-1200 can add more sonic possibilities and expand the range of drums sounds\n"
     ]
    }
   ],
   "source": [
    "# encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "encodeds = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", ).to(device)\n",
    "print(encodeds)\n",
    "attention_mask = torch.ones_like(encodeds)\n",
    "generated_ids = model.generate(encodeds, attention_mask=attention_mask , max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as above but without direct tokenization using the chat template...does make a minor difference to the output but can be more flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     1,   733, 16289, 28793,  1602,   654,   272, 11714,   297,\n",
      "           272,   668, 28740, 28750, 28734, 28734, 23174, 28804,   733, 28748,\n",
      "         16289, 28793,  1014, 11714,   654, 23174,   297, 28705, 28740, 28750,\n",
      "         28733,  2581,  9596, 28755, 28723,     2,   733, 16289, 28793,  1824,\n",
      "           829,   347,  2203,   298,  4916,   272, 16049,  7258, 28804,   733,\n",
      "         28748, 16289, 28793]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST]There are several ways to improve drum sounds in a SP-1200 or any other sampler:\n",
      "\n",
      "1. Use high-quality source material: Always start with the best possible recordings or sounds for your drum samples.\n",
      "\n",
      "2. Apply EQ: Boost or cut frequencies to make the drums sit better in the mix, add warmth, and improve definition.\n",
      "\n",
      "3. Use compression: Compression can help even out the dynamics of the drums and add consistency to the sounds.\n",
      "\n",
      "4. Apply effects: Apply reverb, delay, distortion, or other effects to add depth, character, and interest to the drum sounds.\n",
      "\n",
      "5. Edit and prepare the samples: Use techniques like normalization, resampling, and layering to edit and prepare the samples to fit your needs.\n",
      "\n",
      "6. Adjust the sample rate and bit depth: Upsampling or downsampling the samples may improve their quality or fit them better with the rest of your project.\n",
      "\n",
      "7. Use external processing: Connect external effects processors or synthesizers to the SP-1200 to add additional processing options and expand the possibilities for creating unique drum sounds.\n",
      "\n",
      "8. Practice good recording and mixing\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.2 s ± 304 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r 2 model.generate(**inputs, max_new_tokens=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 16.44 t/s: 18.80\n",
      "duration: 16.49 t/s: 18.74\n",
      "Averages for prompt and generated | duration: 16.47 t/s: 18.77\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_repeats = 2\n",
    "total_duration = 0\n",
    "total_tps = 0\n",
    "for _ in range(n_repeats):\n",
    "    start = time.time()\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    total_duration += duration\n",
    "    total_tps += len(generated_ids[0]) / duration\n",
    "    print(f\"duration: {duration:.2f}, t/s: {len(generated_ids[0]) / duration:.2f}\")\n",
    "print(f\"Averages for prompt and generated | duration: {total_duration / n_repeats:.2f}, t/s: {total_tps / n_repeats:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this ignores the difference between prompt and generated tokens, but as long as we're comparing like for like\n",
    "# it's good enough for these purposes. Should find the time for each part separately for a more accurate comparison.\n",
    "def calculate_average_duration(inputs, model, n_repeats=2, max_new_tokens=256):\n",
    "    total_duration = 0\n",
    "    total_tps = 0\n",
    "    for _ in range(n_repeats):\n",
    "        start = time.time()\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        total_duration += duration\n",
    "        total_tps += len(generated_ids[0]) / duration\n",
    "        print(f\"duration: {duration:.2f}, t/s: {len(generated_ids[0]) / duration:.2f}\")\n",
    "    print(f\"Averages for prompt and generated | duration: {total_duration / n_repeats:.2f}, t/s: {total_tps / n_repeats:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_average_duration(inputs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test for 2 repeats on the inputs defined above gives:\n",
    "\n",
    "- duration: 16.44 t/s: 18.80\n",
    "- duration: 16.49 t/s: 18.74\n",
    "- Averages for prompt and generated | duration: 16.47 t/s: 18.77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# check that the variable `model` exists and if it does, delete it\n",
    "if \"model\" in locals():\n",
    "    model = model.to(\"cpu\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] How were the samples in the sp1200 encoded? [/INST]The samples were encoded in 12-bit PCM. [INST] What could be done to improve the drum sounds? [/INST] There are several ways to improve drum sounds using the SP-1200 or any other digital sampling workstation:\n",
      "\n",
      "1. Sample selection: Choosing high-quality source material to begin with is crucial. High-end microphones, pre-amps, and interfaces can make a significant difference in the final sound.\n",
      "\n",
      "2. Sample editing: Manipulating the samples to tailor them to your needs. This includes trimming, pitch shifting, reverse, and time-stretching.\n",
      "\n",
      "3. Sample processing: Applying various effects to enhance or change the sound. Equalization, compression, and reverb are some of the most common effects.\n",
      "\n",
      "4. Sequencing: Arranging the sounds in a pleasing way to create a groove or rhythm.\n",
      "\n",
      "5. Layering: Combining multiple samples to create complex drum sounds and textures.\n",
      "\n",
      "6. Tuning and timing: Making sure the drum sounds are in tune and in time.\n",
      "\n",
      "7. MIDI editing: Converting drum sounds to MIDI and editing them within a Digital Audio Workstation (DAW) can allow for further customization and real-time editing.\n",
      "\n",
      "8. Automation: Using the autom\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 20.21, t/s: 15.29\n",
      "duration: 20.23, t/s: 15.27\n",
      "Averages for prompt and generated | duration: 20.22, t/s: 15.28\n"
     ]
    }
   ],
   "source": [
    "calculate_average_duration(inputs, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
